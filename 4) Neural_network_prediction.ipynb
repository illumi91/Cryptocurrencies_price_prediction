{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the lenght of the sequence (SEQ_LEN) that we will be using to predict a certain cryptocurrency (RATIO_TO_PREDICT) into a future time (FUTURE_PERIOD_PREDICT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 72  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict\n",
    "RATIO_TO_PREDICT = \"ETHTUSD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as we did for machine learning we will be creating our labels based on a % change in prices for our cryptocurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_sell_hold(future):\n",
    "    '''Return 1, 0, 2 based on % change\n",
    "    \n",
    "       Args:\n",
    "           future: price timeseries lagged into future\n",
    "    '''\n",
    "    change_perc = 0.01\n",
    "    \n",
    "    if future > change_perc:\n",
    "        return 1\n",
    "    if future < -change_perc:\n",
    "        return 0\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    '''Define labels X, y creating a column containing the values of the price difference normalized\n",
    "       based on the hours we want to predict in future\n",
    "    \n",
    "       Args:\n",
    "           df: df containing closing prices for cryptocurrencies we want to predict\n",
    "           \n",
    "       Returns:\n",
    "           np.array(X): sequences we will use as feature to predict\n",
    "           y: target variable\n",
    "    '''\n",
    "    df = df.drop(\"pct_change\", 1)  # don't need this anymore.\n",
    "    for col in df.columns:  # go through all of the columns\n",
    "        if col != \"target\":  # normalize all ... except for the target itself!\n",
    "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
    "            df.fillna(method='ffill')  # remove the nas created by pct_change\n",
    "            df = df.replace([np.inf, -np.inf], 0)\n",
    "            df.fillna(0, inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
    "\n",
    "    df.dropna(inplace=True)  # cleanup again... \n",
    "\n",
    "\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. \n",
    "    \n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "    \n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "\n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "    holds = []  # list that will store our sell sequences and targets\n",
    "    \n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 0:  # if it's a \"not buy\"\n",
    "            sells.append([seq, target])  # append to sells list\n",
    "        elif target == 1:  # otherwise if the target is a 1...\n",
    "            buys.append([seq, target])  # it's a buy!\n",
    "        else:\n",
    "            holds.append([seq, target])\n",
    "    \n",
    "    random.shuffle(buys)  # shuffle the buys\n",
    "    random.shuffle(sells)  # shuffle the sells!\n",
    "    random.shuffle(holds)  # shuffle the holds!\n",
    "    \n",
    "    lower = min(len(buys), len(sells), len(holds))  # what's the shorter length?\n",
    "    \n",
    "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
    "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
    "    holds = holds[:lower]  # make sure both lists are only up to the shortest length.\n",
    "    \n",
    "    sequential_data = buys+sells+holds  # add them together\n",
    "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy vs holds)\n",
    "\n",
    "    return np.array(X), y  # return X and y...and make X a numpy array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define 5 symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = ['BTCTUSD', \"ETHTUSD\" , \"XRPTUSD\", \"LTCTUSD\", \"EOSTUSD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create main df with close prices and volume for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTCTUSD\n",
      "ETHTUSD\n",
      "XRPTUSD\n",
      "LTCTUSD\n",
      "EOSTUSD\n",
      "               BTCTUSD_close  BTCTUSD_volume  ETHTUSD_close  ETHTUSD_volume  \\\n",
      "Time                                                                          \n",
      "1581346800000        9914.98       42.847840         224.48       437.23191   \n",
      "1581343200000        9867.88       10.544541         223.24       921.30562   \n",
      "1581339600000        9811.44       12.148842         219.41       672.55340   \n",
      "1581336000000        9838.86       10.340284         219.73       279.35173   \n",
      "1581332400000        9807.88       22.957431         217.91       203.97147   \n",
      "\n",
      "               XRPTUSD_close  XRPTUSD_volume  LTCTUSD_close  LTCTUSD_volume  \\\n",
      "Time                                                                          \n",
      "1581346800000        0.27595        150520.2          74.87      1236.85021   \n",
      "1581343200000        0.27370         20433.0          74.44       300.63211   \n",
      "1581339600000        0.27156          9748.0          73.41       227.99520   \n",
      "1581336000000        0.27188         13476.4          73.49        91.53713   \n",
      "1581332400000        0.27007          9632.8          73.05       267.58230   \n",
      "\n",
      "               EOSTUSD_close  EOSTUSD_volume  \n",
      "Time                                          \n",
      "1581346800000         4.9057          138.60  \n",
      "1581343200000         4.9100           20.00  \n",
      "1581339600000         4.8321          760.33  \n",
      "1581336000000         4.8150          176.16  \n",
      "1581332400000         4.7682          523.45  \n"
     ]
    }
   ],
   "source": [
    "crp_df = pd.DataFrame() # begin empty\n",
    "\n",
    "for ratio in ratios:  # begin iteration\n",
    "    print(ratio)\n",
    "    dataset = f'crypto_dfs/{ratio}.csv'  # get the full path to the file.\n",
    "    \n",
    "    df = pd.read_csv(dataset, index_col=0)  # read in specific file\n",
    "\n",
    "    # rename volume and close to include the ticker so we can still which close/volume is which:\n",
    "    df.rename(columns={\"Close\": f\"{ratio}_close\", \"Volume\": f\"{ratio}_volume\"}, inplace=True)\n",
    "\n",
    "    df.set_index(\"Time\", inplace=True)  # set time as index so we can join them on this shared time\n",
    "    \n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]  # ignore the other columns besides price and volume\n",
    "\n",
    "    if len(crp_df)==0:  # if the dataframe is empty\n",
    "        crp_df = df  # then it's just the current df\n",
    "    else:  # otherwise, join this data to the main one\n",
    "        crp_df = crp_df.join(df)\n",
    "\n",
    "print(crp_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag price into future through pct_change.\n",
    "\n",
    "In our case 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_change</th>\n",
       "      <th>ETHTUSD_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1581346800000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>224.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581343200000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>223.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581339600000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>219.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581336000000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>219.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581332400000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>217.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pct_change  ETHTUSD_close\n",
       "Time                                    \n",
       "1581346800000         NaN         224.48\n",
       "1581343200000         NaN         223.24\n",
       "1581339600000         NaN         219.41\n",
       "1581336000000         NaN         219.73\n",
       "1581332400000         NaN         217.91"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crp_df['pct_change'] = crp_df[[f'{RATIO_TO_PREDICT}_close']].pct_change(24)\n",
    "crp_df[['pct_change', f'{RATIO_TO_PREDICT}_close']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply our previous function to get our target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crp_df['target'] = crp_df['pct_change'].map(buy_sell_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.350962\n",
       "1    0.326122\n",
       "2    0.322917\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crp_df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split df in 2 parts for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(crp_df.index.values)  # get the times\n",
    "last_20pct = sorted(crp_df.index.values)[-int(0.2*len(times))]  # get the last 20% of the times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_main_df = crp_df[(crp_df.index >= last_20pct)]  # make the validation data where the index is in the last 5%\n",
    "main_df = crp_df[(crp_df.index < last_20pct)]  # now the main_df is all the data up to the last 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1779 Validation: 282\n",
      "Sells: 593, Holds: 593, Buys: 593\n",
      "VALIDATION Sells: 94, Holds: 94, buys: 94\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "print(f\"Train data: {len(train_x)} Validation: {len(validation_x)}\")\n",
    "print(f\"Sells: {train_y.count(0)}, Holds: {train_y.count(2)}, Buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Sells: {validation_y.count(0)}, Holds: {validation_y.count(2)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a few more constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"  # a unique name for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(3, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=f\"logs/{NAME}\")\n",
    "\n",
    "filepath = \"LSTM_Final-{epoch:02d}-{val_acc:.3f}\"\n",
    "# unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) \n",
    "# saves only the best ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1779 samples, validate on 282 samples\n",
      "Epoch 1/10\n",
      "1779/1779 [==============================] - 11s 6ms/step - loss: 3.2527 - acc: 0.5587 - val_loss: 3.2109 - val_acc: 0.5035\n",
      "Epoch 2/10\n",
      "1779/1779 [==============================] - 11s 6ms/step - loss: 2.0801 - acc: 0.5767 - val_loss: 2.4847 - val_acc: 0.5213\n",
      "Epoch 3/10\n",
      "1779/1779 [==============================] - 12s 7ms/step - loss: 2.9954 - acc: 0.5301 - val_loss: 1.8605 - val_acc: 0.4858\n",
      "Epoch 4/10\n",
      "1779/1779 [==============================] - 13s 7ms/step - loss: 2.2292 - acc: 0.5565 - val_loss: 2.2737 - val_acc: 0.5177\n",
      "Epoch 5/10\n",
      "1779/1779 [==============================] - 13s 7ms/step - loss: 2.1662 - acc: 0.5312 - val_loss: 2.0370 - val_acc: 0.4681\n",
      "Epoch 6/10\n",
      "1779/1779 [==============================] - 13s 7ms/step - loss: 1.9661 - acc: 0.5391 - val_loss: 2.1765 - val_acc: 0.5071\n",
      "Epoch 7/10\n",
      "1779/1779 [==============================] - 12s 7ms/step - loss: 1.7820 - acc: 0.6037 - val_loss: 2.1473 - val_acc: 0.5248\n",
      "Epoch 8/10\n",
      "1779/1779 [==============================] - 14s 8ms/step - loss: 1.7578 - acc: 0.5835 - val_loss: 2.3048 - val_acc: 0.4929\n",
      "Epoch 9/10\n",
      "1779/1779 [==============================] - 14s 8ms/step - loss: 1.4955 - acc: 0.5627 - val_loss: 2.5102 - val_acc: 0.4929\n",
      "Epoch 10/10\n",
      "1779/1779 [==============================] - 14s 8ms/step - loss: 1.4352 - acc: 0.5857 - val_loss: 2.2426 - val_acc: 0.5248\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.2425889157234353\n",
      "Test accuracy: 0.5248226967263729\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"models/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison table between algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Algorithm            | Accuracy |  \n",
    "|:-------------:       |------:        |  \n",
    "| Neural network       |    0.60        |    \n",
    "|  Logistic regression | 0.34        |\n",
    "|    Decision tree     |   0.38        | \n",
    "| Random forest        |    0.48        |   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
